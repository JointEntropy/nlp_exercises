{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e66d6304",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "-  ~~ pass batch instead of singal sequence entity~~\n",
    "- test on some toy example.\n",
    "\n",
    "- attention mask\n",
    "\n",
    "- implement positional embeddings (maybe in another notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6481e7c5",
   "metadata": {},
   "source": [
    "sources: \n",
    "\n",
    "https://jalammar.github.io/illustrated-transformer/\n",
    "\n",
    "https://auro-227.medium.com/writing-a-custom-layer-in-pytorch-14ab6ac94b77\n",
    "\n",
    "https://github.com/karpathy/minGPT/blob/master/mingpt/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10c2b101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "7e9e1f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb133d80",
   "metadata": {},
   "source": [
    "### SelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "325dd675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Attention(torch.nn.Module):\n",
    "    def __init__(self, emb_size, kdim=None, vdim=None):\n",
    "        \"\"\"\n",
    "        emb_size - size of input embedding vector\n",
    "        by default w_size(aka k_dim, v_dim equal to emb_size) in https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html#torch.nn.MultiheadAttention\n",
    "        \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.emb_size  = emb_size\n",
    "        \n",
    "        # Inner dimensions of Q and K matrices are equal, dimension of V might be different. \n",
    "        # ...but by deafault all of them are equal to emb_size\n",
    "        self.kdim  = kdim or emb_size\n",
    "        self.vdim  = vdim or emb_size\n",
    "#         self.W_Q = nn.Parameter(torch.Tensor(self.emb_size, self.kdim))\n",
    "#         self.W_K = nn.Parameter(torch.Tensor(self.emb_size, self.kdim))\n",
    "#         self.W_V = nn.Parameter(torch.Tensor(self.emb_size, self.vdim))\n",
    "        self.W_Q = nn.Linear(self.emb_size, self.kdim)\n",
    "        self.W_K = nn.Linear(self.emb_size, self.kdim)\n",
    "        self.W_V = nn.Linear(self.emb_size, self.vdim)\n",
    "        \n",
    "    def forward(self, X: torch.Tensor):\n",
    "        \"\"\"\n",
    "        The first step is to calculate the Query, Key, and Value matrices. \n",
    "        We do that by packing our embeddings into a matrix X, and multiplying it by the weight \n",
    "        matrices weâ€™ve trained (WQ, WK, WV).\n",
    "        \n",
    "        - Every row in the X matrix corresponds to a word in the input sentence. \n",
    "        \n",
    "        \"\"\"\n",
    "#         Q = torch.matmul(X, self.W_Q)\n",
    "#         K = torch.matmul(X, self.W_K)\n",
    "#         V = torch.matmul(X, self.W_V)\n",
    "        Q = self.W_Q(X)\n",
    "        K = self.W_K(X)\n",
    "        V = self.W_V(X)\n",
    "        numerator = torch.matmul(Q, torch.transpose(K, 1, 2))\n",
    "        denominator = (self.kdim)**0.5\n",
    "        \n",
    "        fraction = torch.softmax(numerator/denominator, axis=-1)\n",
    "        self_attention = torch.matmul(fraction, V)\n",
    "        return self_attention\n",
    "    \n",
    "emb_size = 20\n",
    "kdim = 12\n",
    "vdim = 11\n",
    "layer = Attention(emb_size=emb_size, kdim=kdim, vdim=vdim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "1f40e859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 27, 11])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = 27\n",
    "batch_size=100\n",
    "\n",
    "X = torch.Tensor(1, seq_len, emb_size).random_()\n",
    "layer(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c50255f",
   "metadata": {},
   "source": [
    "### MultiHeadAttention\n",
    "\n",
    "todo implement general attention version of this not just self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "987227b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 47, 33])"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, n_heads, emb_size, vdim=None, kdim=None):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.emb_size  = emb_size\n",
    "        self.kdim  = kdim or emb_size\n",
    "        self.vdim  = vdim or emb_size\n",
    "        assert (self.vdim % self.n_heads)==0, 'emb_size must be divisible by n_heads'\n",
    "#         self.W_0 = nn.Parameter(torch.Tensor(self.emb_size, self.vdim))\n",
    "        self.W_0 = nn.Linear(self.vdim, self.vdim, bias=False)\n",
    "        self.heads = [Attention(self.emb_size, vdim=self.vdim//self.n_heads, \n",
    "                                kdim=self.kdim) for i in range(self.n_heads)]\n",
    "        self.ffn = nn.ReLU(nn.Linear(10, 10))\n",
    "        \n",
    "    def forward(self, X: torch.Tensor):\n",
    "        attentions = []\n",
    "        # TODO this can and should be done in parallel\n",
    "        for head in self.heads:\n",
    "            z_i = head(X)\n",
    "            attentions.append(z_i)\n",
    "        Z = torch.concatenate(attentions, axis=-1)\n",
    "        multihead_attention = self.W_0(Z)\n",
    "        output = self.ffn(multihead_attention)\n",
    "        return output\n",
    "\n",
    "    \n",
    "n_heads = 3\n",
    "emb_size = 33\n",
    "multihead_layer = MultiHeadAttention(n_heads=n_heads,  emb_size=emb_size)#, vdim=30)\n",
    "\n",
    "seq_len = 47\n",
    "batch_size=50\n",
    "X = torch.Tensor(batch_size, seq_len, emb_size).random_()\n",
    "multihead_layer(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "cc41e179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = torch.nn.MultiheadAttention(embed_dim=40, num_heads=3, vdim=3)\n",
    "# seq_len = 101\n",
    "# emb_size=40\n",
    "\n",
    "# X = torch.Tensor(seq_len, emb_size).random_()\n",
    "\n",
    "# Q, K, V = X, X, X[:, ]\n",
    "\n",
    "# t(Q, K, V)[0].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
