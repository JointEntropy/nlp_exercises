{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae85db3b",
   "metadata": {},
   "source": [
    "# todo\n",
    "- Проверить на задаче классификации отзывов что получаемые эмбединги текстов - норм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfe08866",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d8caced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b030f6",
   "metadata": {},
   "source": [
    "# Masking language modelling MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85bf82f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting masking_language_modelling/models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile masking_language_modelling/models.py\n",
    "\n",
    "import torch\n",
    "\n",
    "from multihead_attention import MultiHeadAttention\n",
    "\n",
    "class BaseEncoderModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, n_heads, emb_size, vdim=None, kdim=None, padding_idx=None):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx),\n",
    "            MultiHeadAttention(n_heads=n_heads, emb_size=emb_size),\n",
    "            MultiHeadAttention(n_heads=n_heads, emb_size=emb_size),\n",
    "        )\n",
    "    def forward(self, X: torch.Tensor):\n",
    "        return self.layers(X)\n",
    "\n",
    "\n",
    "class MLMHead(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        out_features = self.model.layers[-1].ffn.inplace.out_features\n",
    "        vocab_size = self.model.layers[0].num_embeddings\n",
    "        self.mlm_layer = torch.nn.Linear(out_features, vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self, X: torch.Tensor): #, masked_tokens: torch.Tensor): \n",
    "        state = self.model(X)\n",
    "        logits = self.mlm_layer(state)\n",
    "        # you can't truncate only masked tokens here because the shape of the bantch will be broken.\n",
    "        #         masked_tokens_logits = logits[:, masked_tokens, :]\n",
    "#         result = torch.softmax(logits, axis=-1) # will add this in loss\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3af277f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 7])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, seq_len, emb_size = 2, 3, 7\n",
    "vocab_size = 3\n",
    "torch.nn.Embedding(vocab_size, emb_size)(torch.randint(vocab_size, size=(batch_size, seq_len))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41eb1878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 47, 1000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from masking_language_modelling.models import BaseEncoderModel, MLMHead\n",
    "from masking_language_modelling.dataproc import MLMDataset\n",
    "# batch_size, seq_len, emb_size = 11, 30, 36\n",
    "# X = torch.Tensor(batch_size, seq_len, emb_size).random_()\n",
    "\n",
    "\n",
    "\n",
    "batch_size, seq_len, emb_size = 11, 47, 36\n",
    "vocab_size = 1000\n",
    "base_model = BaseEncoderModel(vocab_size=vocab_size, n_heads=12, emb_size=emb_size)\n",
    "model = MLMHead(model=base_model)\n",
    "\n",
    "X = torch.randint(vocab_size, size=(batch_size, seq_len))\n",
    "probs = model(X)\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26462494",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b22a1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting masking_language_modelling/dataproc.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile masking_language_modelling/dataproc.py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from typing import List, Optional\n",
    "\n",
    "class MLMDataset(Dataset):\n",
    "    def __init__(self, text_fpath: str, \n",
    "                 max_seq_len: int, \n",
    "                 mask_ratio:float = 0.15\n",
    "                ):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.mask_ratio = mask_ratio\n",
    "        with open(text_fpath, 'r') as f:\n",
    "            self.lines = f.readlines()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.lines[idx]\n",
    "    \n",
    "class Tokenizer:\n",
    "    def __init__(self, \n",
    "                 max_vocab_size: int,\n",
    "                 truncation: bool = True,\n",
    "                 max_seq_len: Optional[int] = None,\n",
    "                 padding: bool = True,\n",
    "                ):\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.mask_token = '<MASK>'\n",
    "        self.cls_token = '<CLS>'\n",
    "        self.sep_token = '<SEP>'\n",
    "        self.special_tokens = [self.cls_token, self.sep_token, self.pad_token, self.mask_token]\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.truncation = truncation\n",
    "        self.padding = padding\n",
    "        \n",
    "        if self.padding or self.truncation:\n",
    "            assert not(max_seq_len is None)\n",
    "            self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def fit(self, dataset):\n",
    "        most_common_words = Counter(chain.from_iterable(map(self._preproc, dataset)))\\\n",
    "                                                     .most_common(self.max_vocab_size-len(self.special_tokens))\n",
    "        most_common_words = list(map(lambda x: x[0], most_common_words))\n",
    "        self.vocab = dict(map(lambda x: (x[1], x[0]), enumerate(self.special_tokens + most_common_words)))\n",
    "        return self\n",
    "    \n",
    "    def apply(self, text: str):\n",
    "        pad_token_idx = self.vocab[self.pad_token]\n",
    "        input_seq = self._preproc(text)[:self.max_seq_len]\n",
    "        payload_tokens = list(map(lambda x: self.vocab.get(x, pad_token_idx), input_seq))\n",
    "        padding_tokens = [pad_token_idx]*(self.max_seq_len-len(payload_tokens))\n",
    "        return [self.vocab[self.cls_token]] + payload_tokens + padding_tokens\n",
    "    \n",
    "    def _preproc(self, text: str) -> List[str]:\n",
    "        return text.split()\n",
    "    \n",
    "    \n",
    "def spawn_collate_fn(tokenizer, mask_ratio=0.15):\n",
    "    cls_token_id = tokenizer.vocab[tokenizer.cls_token]\n",
    "    sep_token_id = tokenizer.vocab[tokenizer.sep_token]\n",
    "    mask_token_id = tokenizer.vocab[tokenizer.mask_token]\n",
    "    \n",
    "    def mask_objective(batch_token_ids, mask_ratio):\n",
    "            masked_tokens = torch.rand(batch_token_ids.shape)<mask_ratio\n",
    "            mask_arr = masked_tokens * (batch_token_ids != cls_token_id) * (batch_token_ids != sep_token_id)\n",
    "            return mask_arr\n",
    "        \n",
    "    def custom_collate_fn(batch):\n",
    "        input_ids = torch.Tensor(batch).long()\n",
    "        mlm_mask = mask_objective(input_ids, mask_ratio)\n",
    "        masked_input_ids = torch.where(mlm_mask, mask_token_id, input_ids).long()\n",
    "\n",
    "        return {\n",
    "            'input_tokens': input_ids,\n",
    "            'masked_input_tokens': masked_input_ids,\n",
    "#             'attention_mask': 1, # is this the same as mlm mask?\n",
    "            'mlm_mask': mlm_mask,\n",
    "        }\n",
    "    return custom_collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e847c06",
   "metadata": {},
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d54d506f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.txt\n",
    "kek\n",
    "mda ok na\n",
    "aagaaa\n",
    "kek\n",
    "mda ok na\n",
    "aagaaa\n",
    "kek\n",
    "mda ok na\n",
    "aagaaa\n",
    "kek\n",
    "mda ok na\n",
    "aagaaa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b88f9f5",
   "metadata": {},
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7983054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from masking_language_modelling.dataproc import MLMDataset, spawn_collate_fn, Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "seq_len = 30\n",
    "batch_size = 50\n",
    "max_vocab_size = 30000\n",
    "\n",
    "training_data = MLMDataset('rt.txt', seq_len)\n",
    "tokenizer = Tokenizer(max_vocab_size=max_vocab_size, max_seq_len=seq_len)\\\n",
    "                        .fit(training_data)\n",
    "\n",
    "# print(tokenizer.vocab)\n",
    "# print(tokenizer.apply(f'kek mda {tokenizer.mask_token}'))\n",
    "proc_train_dataset = list(map(tokenizer.apply, training_data))\n",
    "\n",
    "collate_fn = spawn_collate_fn(tokenizer)\n",
    "train_dataloader = DataLoader(proc_train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                              collate_fn=collate_fn)\n",
    "# next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f19a0c6",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9378fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 54\n",
    "\n",
    "base_model = BaseEncoderModel(n_heads=6, emb_size=emb_size, \n",
    "                              vocab_size=len(tokenizer.vocab),\n",
    "                              padding_idx=tokenizer.vocab[tokenizer.pad_token])\n",
    "model = MLMHead(model=base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e97fa820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4767d1da64a4268967ccebee77de9c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12674ea7916f479aa53ff85c8adea6b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba378d2951d241c0997ebc3e03c244d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17055f163c6a4e55af00a535854592f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2af125b4b7cd44c38d3e3c45eb2b8f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58581530c73b4960843b944079d5fc21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfee678d7487407895ed9797690f582d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a7c8784e42243b49b55d05a935dfc38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5f5154d95534752a66cd127874cc41a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60ab8a776a114f529741b54550c5cd53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55cc3ebd6724baeb8dff2526d941899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aba9d3c6803548a1b104b3a3498f0834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ba0842fd214b5688a72a712cedd50f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35ba6084c6244729baf2747a0bc8acde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9242a29b8d9448fb7882388a75ef2da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d7ff99900084eaf920b40209491e419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c3e85bef11c44b381d1269326cf1875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a6168a49c57427898007f053c6e1d85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34a1850bc5c44ebcbb9be3282e37efc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d46249e9da542f7924e912f7faeb3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938ce2816bc84a68abc85646bfe4b281",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39bb37c434784ef7b7ef8685ca18f421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a779cc14c3254d9191f8e6320166dcc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13206aa77792470595841853fa8a1dac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b333538973c243b396f9a0ec0d86be77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/151 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 30>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m             losses\u001b[38;5;241m.\u001b[39mappend(avg_masked_loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     28\u001b[0m             pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCEL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(losses)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, num_epochs, lr)\u001b[0m\n\u001b[1;32m     21\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     22\u001b[0m loss_res \u001b[38;5;241m=\u001b[39m loss(logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]), y\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m---> 23\u001b[0m masked_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_res\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmasked_tokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m avg_masked_loss \u001b[38;5;241m=\u001b[39m masked_loss\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     25\u001b[0m avg_masked_loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def train(model, train_dataloader, num_epochs=1, lr=1e-3):    \n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        print(f'Epoch {epoch}')\n",
    "        pbar=tqdm(train_dataloader)\n",
    "        for batch in pbar:\n",
    "            # x is tensor with masked tokens [batch_size, seq_len]\n",
    "            x = batch['masked_input_tokens']\n",
    "\n",
    "            # y is tensor without masked_tokens [batch_size, seq_len]\n",
    "            y = batch['input_tokens']\n",
    "\n",
    "            # masked_tokens is boolean tensor with masked tokens [batch_size, seq_len]\n",
    "            masked_tokens = batch['mlm_mask']\n",
    "            model.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss_res = loss(logits.view(-1,logits.shape[2]), y.view(-1))\n",
    "            masked_loss = loss_res*masked_tokens.view(-1)\n",
    "            avg_masked_loss = masked_loss.mean()\n",
    "            avg_masked_loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(avg_masked_loss.detach().numpy())\n",
    "            pbar.set_description(f'CEL: {np.mean(losses):.3f}')\n",
    "        \n",
    "train(model, train_dataloader, num_epochs=100, lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49133f3",
   "metadata": {},
   "source": [
    "sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "56f2d798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>fairytale for yound ladies. miracle fantasy</td>\n",
       "      <td>a resonant tale of racism , revenge and retrib...</td>\n",
       "      <td>0.999852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>946</th>\n",
       "      <td>fairytale for yound ladies. miracle fantasy</td>\n",
       "      <td>it's a remarkably solid and subtly satirical t...</td>\n",
       "      <td>0.999700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>fairytale for yound ladies. miracle fantasy</td>\n",
       "      <td>uno de los policiales más interesantes de los ...</td>\n",
       "      <td>0.999697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>fairytale for yound ladies. miracle fantasy</td>\n",
       "      <td>has a shambling charm . . . a cheerfully incon...</td>\n",
       "      <td>0.999643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>fairytale for yound ladies. miracle fantasy</td>\n",
       "      <td>desta vez , columbus capturou o pomo de ouro .\\n</td>\n",
       "      <td>0.999624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>944</th>\n",
       "      <td>fairytale for yound ladies. miracle fantasy</td>\n",
       "      <td>it moves quickly , adroitly , and without fuss...</td>\n",
       "      <td>0.349197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>fairytale for yound ladies. miracle fantasy</td>\n",
       "      <td>you'll be left with the sensation of having ju...</td>\n",
       "      <td>0.339534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>fairytale for yound ladies. miracle fantasy</td>\n",
       "      <td>leave it to rohmer , now 82 , to find a way to...</td>\n",
       "      <td>0.327612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>fairytale for yound ladies. miracle fantasy</td>\n",
       "      <td>greene delivers a typically solid performance ...</td>\n",
       "      <td>0.305827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>fairytale for yound ladies. miracle fantasy</td>\n",
       "      <td>it's not so much enjoyable to watch as it is e...</td>\n",
       "      <td>0.286589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               a  \\\n",
       "761  fairytale for yound ladies. miracle fantasy   \n",
       "946  fairytale for yound ladies. miracle fantasy   \n",
       "764  fairytale for yound ladies. miracle fantasy   \n",
       "625  fairytale for yound ladies. miracle fantasy   \n",
       "694  fairytale for yound ladies. miracle fantasy   \n",
       "..                                           ...   \n",
       "944  fairytale for yound ladies. miracle fantasy   \n",
       "786  fairytale for yound ladies. miracle fantasy   \n",
       "893  fairytale for yound ladies. miracle fantasy   \n",
       "799  fairytale for yound ladies. miracle fantasy   \n",
       "277  fairytale for yound ladies. miracle fantasy   \n",
       "\n",
       "                                                     b     score  \n",
       "761  a resonant tale of racism , revenge and retrib...  0.999852  \n",
       "946  it's a remarkably solid and subtly satirical t...  0.999700  \n",
       "764  uno de los policiales más interesantes de los ...  0.999697  \n",
       "625  has a shambling charm . . . a cheerfully incon...  0.999643  \n",
       "694   desta vez , columbus capturou o pomo de ouro .\\n  0.999624  \n",
       "..                                                 ...       ...  \n",
       "944  it moves quickly , adroitly , and without fuss...  0.349197  \n",
       "786  you'll be left with the sensation of having ju...  0.339534  \n",
       "893  leave it to rohmer , now 82 , to find a way to...  0.327612  \n",
       "799  greene delivers a typically solid performance ...  0.305827  \n",
       "277  it's not so much enjoyable to watch as it is e...  0.286589  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "\n",
    "\n",
    "def get_embedding(query, tokenizer, base_model):\n",
    "    tokens = tokenizer.apply(query)\n",
    "    return base_model(torch.tensor(tokens).view(1, -1))[0, 0].detach().numpy()\n",
    "\n",
    "queries = list(training_data)[:1000]\n",
    "embs = []\n",
    "base_model.eval()\n",
    "\n",
    "query = 'fairytale for yound ladies. miracle fantasy'\n",
    "query_emb = get_embedding(query, tokenizer, base_model)\n",
    "\n",
    "res_lst = []\n",
    "for entry in queries:\n",
    "    entry_emb = get_embedding(entry, tokenizer, base_model)\n",
    "    score = 1 - spatial.distance.cosine(query_emb, entry_emb)\n",
    "    res_lst.append((query, entry, score))\n",
    "\n",
    "scores_df = pd.DataFrame(res_lst, columns=['a', 'b', 'score'])\n",
    "scores_df.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "17dd50d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.8794)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline loss\n",
    "kek_loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "n, C = 5, len(tokenizer.vocab)\n",
    "\n",
    "a1 = torch.Tensor(np.random.uniform(size=(n, C)))\n",
    "b1 = torch.Tensor(np.random.randint(C, size=n)).long()\n",
    "kek_loss(a1, b1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "69994d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'b': 'a mostly intelligent , engrossing and psychologically resonant suspenser .\\n',\n",
       "  'score': 0.9985803365707397},\n",
       " {'b': 'a pleasant enough movie , held together by skilled ensemble actors .\\n',\n",
       "  'score': 0.9978662729263306},\n",
       " {'b': \"steve irwin's method is ernest hemmingway at accelerated speed and volume .\\n\",\n",
       "  'score': 0.9978216290473938},\n",
       " {'b': \"this is the best american movie about troubled teens since 1998's whatever .\\n\",\n",
       "  'score': 0.9976030588150024},\n",
       " {'b': \"cantet perfectly captures the hotel lobbies , two-lane highways , and roadside cafes that permeate vincent's days\\n\",\n",
       "  'score': 0.9975119233131409},\n",
       " {'b': 'what really surprises about wisegirls is its low-key quality and genuine tenderness .\\n',\n",
       "  'score': 0.9969547390937805},\n",
       " {'b': 'an idealistic love story that brings out the latent 15-year-old romantic in everyone .\\n',\n",
       "  'score': 0.9969406723976135},\n",
       " {'b': 'guaranteed to move anyone who ever shook , rattled , or rolled .\\n',\n",
       "  'score': 0.9968717098236084},\n",
       " {'b': 'offers a breath of the fresh air of true sophistication .\\n',\n",
       "  'score': 0.9967560768127441},\n",
       " {'b': 'gosling provides an amazing performance that dwarfs everything else in the film .\\n',\n",
       "  'score': 0.9967530965805054},\n",
       " {'b': 'one of the greatest family-oriented , fantasy-adventure movies ever .\\n',\n",
       "  'score': 0.9966621398925781},\n",
       " {'b': 'a disturbing and frighteningly evocative assembly of imagery and hypnotic music composed by philip glass .\\n',\n",
       "  'score': 0.9962896704673767},\n",
       " {'b': 'ms . fulford-wierzbicki is almost spooky in her sulky , calculating lolita turn .\\n',\n",
       "  'score': 0.9961518049240112},\n",
       " {'b': 'a romantic comedy that operates by the rules of its own self-contained universe .\\n',\n",
       "  'score': 0.9960781335830688},\n",
       " {'b': 'haneke challenges us to confront the reality of sexual aberration .\\n',\n",
       "  'score': 0.9957752227783203},\n",
       " {'b': 'take care of my cat offers a refreshingly different slice of asian cinema .\\n',\n",
       "  'score': 0.9955235719680786},\n",
       " {'b': 'chicago is sophisticated , brash , sardonic , completely joyful in its execution .\\n',\n",
       "  'score': 0.9952717423439026},\n",
       " {'b': 'in its ragged , cheap and unassuming way , the movie works .\\n',\n",
       "  'score': 0.9952492117881775},\n",
       " {'b': \"the movie's ripe , enrapturing beauty will tempt those willing to probe its inscrutable mysteries .\\n\",\n",
       "  'score': 0.9949316382408142},\n",
       " {'b': 'ultimately , it ponders the reasons we need stories so much .\\n',\n",
       "  'score': 0.9936740398406982},\n",
       " {'b': 'the animated subplot keenly depicts the inner struggles of our adolescent heroes - insecure , uncontrolled , and intense .\\n',\n",
       "  'score': 0.9922001957893372},\n",
       " {'b': 'a feel-good picture in the best sense of the term .\\n',\n",
       "  'score': 0.9921684861183167},\n",
       " {'b': 'if nothing else , this movie introduces a promising , unusual kind of psychological horror .\\n',\n",
       "  'score': 0.9919818639755249},\n",
       " {'b': 'the invincible werner herzog is alive and well and living in la\\n',\n",
       "  'score': 0.9919776320457458},\n",
       " {'b': 'a few artsy flourishes aside , narc is as gritty as a movie gets these days .\\n',\n",
       "  'score': 0.9911141991615295},\n",
       " {'b': \"like the film's almost anthropologically detailed realization of early-'80s suburbia , it's significant without being overstated .\\n\",\n",
       "  'score': 0.9880995750427246},\n",
       " {'b': 'scores a few points for doing what it does with a dedicated and good-hearted professionalism .\\n',\n",
       "  'score': 0.9871717095375061},\n",
       " {'b': \"disney has always been hit-or-miss when bringing beloved kids' books to the screen . . . tuck everlasting is a little of both .\\n\",\n",
       "  'score': 0.986152708530426},\n",
       " {'b': 'fuller would surely have called this gutsy and at times exhilarating movie a great yarn .\\n',\n",
       "  'score': 0.9858570694923401},\n",
       " {'b': 'this is a film well worth seeing , talking and singing heads and all .\\n',\n",
       "  'score': 0.981582760810852},\n",
       " {'b': 'manages to be original , even though it rips off many of its ideas .\\n',\n",
       "  'score': 0.9815595746040344},\n",
       " {'b': 'perhaps no picture ever made has more literally showed that the road to hell is paved with good intentions .\\n',\n",
       "  'score': 0.9784995913505554},\n",
       " {'b': \"hip-hop has a history , and it's a metaphor for this love story .\\n\",\n",
       "  'score': 0.9773191213607788},\n",
       " {'b': 'it helps that lil bow wow . . . tones down his pint-sized gangsta act to play someone who resembles a real kid .\\n',\n",
       "  'score': 0.9768785238265991},\n",
       " {'b': \"if it's possible for a sequel to outshine the original , then sl2 does just that .\\n\",\n",
       "  'score': 0.9767757654190063},\n",
       " {'b': 'the values that have held the enterprise crew together through previous adventures and perils do so again-courage , self-sacrifice and patience under pressure .\\n',\n",
       "  'score': 0.9761520028114319},\n",
       " {'b': 'the film makes a strong case for the importance of the musicians in creating the motown sound .\\n',\n",
       "  'score': 0.9760218858718872},\n",
       " {'b': 'at heart the movie is a deftly wrought suspense yarn whose richer shadings work as coloring rather than substance .\\n',\n",
       "  'score': 0.9742759466171265},\n",
       " {'b': 'while the isle is both preposterous and thoroughly misogynistic , its vistas are incredibly beautiful to look at .\\n',\n",
       "  'score': 0.9738774299621582},\n",
       " {'b': \"the film is often filled with a sense of pure wonderment and excitement not often seen in today's cinema du sarcasm\\n\",\n",
       "  'score': 0.9735401272773743},\n",
       " {'b': 'just the labour involved in creating the layered richness of the imagery in this chiaroscuro of madness and light is astonishing .\\n',\n",
       "  'score': 0.9728451371192932},\n",
       " {'b': \"emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\\n\",\n",
       "  'score': 0.9720697999000549},\n",
       " {'b': 'part of the charm of satin rouge is that it avoids the obvious with humour and lightness .\\n',\n",
       "  'score': 0.969085156917572},\n",
       " {'b': 'though everything might be literate and smart , it never took off and always seemed static .\\n',\n",
       "  'score': 0.96759033203125},\n",
       " {'b': 'a masterful film from a master filmmaker , unique in its deceptive grimness , compelling in its fatalist worldview .\\n',\n",
       "  'score': 0.9669777750968933},\n",
       " {'b': 'this is a startling film that gives you a fascinating , albeit depressing view of iranian rural life close to the iraqi border .\\n',\n",
       "  'score': 0.9660180807113647},\n",
       " {'b': 'son of the bride may be a good half-hour too long but comes replete with a flattering sense of mystery and quietness .\\n',\n",
       "  'score': 0.9640785455703735},\n",
       " {'b': 'behind the snow games and lovable siberian huskies ( plus one sheep dog ) , the picture hosts a parka-wrapped dose of heart .\\n',\n",
       "  'score': 0.9627302289009094},\n",
       " {'b': 'this delicately observed story , deeply felt and masterfully stylized , is a triumph for its maverick director .\\n',\n",
       "  'score': 0.9626487493515015},\n",
       " {'b': \"you'd think by now america would have had enough of plucky british eccentrics with hearts of gold . yet the act is still charming here .\\n\",\n",
       "  'score': 0.960833728313446},\n",
       " {'b': 'newton draws our attention like a magnet , and acts circles around her better known co-star , mark wahlberg .\\n',\n",
       "  'score': 0.9595275521278381},\n",
       " {'b': 'sandra nettelbeck beautifully orchestrates the transformation of the chilly , neurotic , and self-absorbed martha as her heart begins to open .\\n',\n",
       "  'score': 0.9582711458206177},\n",
       " {'b': \"a sports movie with action that's exciting on the field and a story you care about off it .\\n\",\n",
       "  'score': 0.9568820595741272},\n",
       " {'b': 'writer-director burger imaginatively fans the embers of a dormant national grief and curiosity that has calcified into chronic cynicism and fear .\\n',\n",
       "  'score': 0.9562728404998779},\n",
       " {'b': \"'compleja e intelectualmente retadora , el ladrón de orquídeas es uno de esos filmes que vale la pena ver precisamente por su originalidad . '\\n\",\n",
       "  'score': 0.9544730186462402},\n",
       " {'b': 'everytime you think undercover brother has run out of steam , it finds a new way to surprise and amuse .\\n',\n",
       "  'score': 0.944664716720581},\n",
       " {'b': 'a simmering psychological drama in which the bursts of sudden violence are all the more startling for the slow buildup that has preceded them .\\n',\n",
       "  'score': 0.9417919516563416},\n",
       " {'b': 'absorbing and disturbing -- perhaps more disturbing than originally intended -- but a little clarity would have gone a long way .\\n',\n",
       "  'score': 0.9370546936988831},\n",
       " {'b': \"an utterly compelling 'who wrote it' in which the reputation of the most famous author who ever lived comes into question .\\n\",\n",
       "  'score': 0.9265124797821045},\n",
       " {'b': \"it's the best film of the year so far , the benchmark against which all other best picture contenders should be measured .\\n\",\n",
       "  'score': 0.9206358194351196},\n",
       " {'b': 'the movie is a blast of educational energy , as bouncy animation and catchy songs escort you through the entire 85 minutes .\\n',\n",
       "  'score': 0.904319703578949},\n",
       " {'b': 'morton is a great actress portraying a complex character , but morvern callar grows less compelling the farther it meanders from its shocking start .\\n',\n",
       "  'score': 0.8928200006484985},\n",
       " {'b': \"4 friends , 2 couples , 2000 miles , and all the pabst blue ribbon beer they can drink - it's the ultimate redneck road-trip .\\n\",\n",
       "  'score': 0.8884792923927307},\n",
       " {'b': \"if this movie were a book , it would be a page-turner , you can't wait to see what happens next .\\n\",\n",
       "  'score': 0.8847825527191162},\n",
       " {'b': \"not for everyone , but for those with whom it will connect , it's a nice departure from standard moviegoing fare .\\n\",\n",
       "  'score': 0.8767293095588684},\n",
       " {'b': 'a refreshing korean film about five female high school friends who face an uphill battle when they try to take their relationships into deeper waters .\\n',\n",
       "  'score': 0.8652496933937073},\n",
       " {'b': '( wendigo is ) why we go to the cinema : to be fed through the eye , the heart , the mind .\\n',\n",
       "  'score': 0.858326256275177},\n",
       " {'b': \"painful to watch , but viewers willing to take a chance will be rewarded with two of the year's most accomplished and riveting film performances .\\n\",\n",
       "  'score': 0.8501027822494507},\n",
       " {'b': 'a truly moving experience , and a perfect example of how art -- when done right -- can help heal , clarify , and comfort .\\n',\n",
       "  'score': 0.8495346903800964},\n",
       " {'b': 'if you sometimes like to go to the movies to have fun , wasabi is a good place to start .\\n',\n",
       "  'score': 0.8418857455253601},\n",
       " {'b': \"a real movie , about real people , that gives us a rare glimpse into a culture most of us don't know .\\n\",\n",
       "  'score': 0.841178834438324},\n",
       " {'b': 'the film provides some great insight into the neurotic mindset of all comics -- even those who have reached the absolute top of the game .\\n',\n",
       "  'score': 0.816569447517395},\n",
       " {'b': \"may be spoofing an easy target -- those old '50's giant creature features -- but . . . it acknowledges and celebrates their cheesiness as the reason why people get a kick out of watching them today .\\n\",\n",
       "  'score': 0.8155146837234497},\n",
       " {'b': \"if there's a way to effectively teach kids about the dangers of drugs , i think it's in projects like the ( unfortunately r-rated ) paid .\\n\",\n",
       "  'score': 0.794975996017456},\n",
       " {'b': 'in a normal screen process , these bromides would be barely enough to sustain an interstitial program on the discovery channel . but in imax 3-d , the clichés disappear into the vertiginous perspectives opened up by the photography .\\n',\n",
       "  'score': 0.7757589817047119},\n",
       " {'b': \"it's this memory-as-identity obviation that gives secret life its intermittent unease , reaffirming that long-held illusions are indeed reality , and that erasing them recasts the self .\\n\",\n",
       "  'score': 0.739637553691864},\n",
       " {'b': \"while mcfarlane's animation lifts the film firmly above the level of other coming-of-age films . . . it's also so jarring that it's hard to get back into the boys' story .\\n\",\n",
       "  'score': 0.7155584692955017},\n",
       " {'b': 'though it is by no means his best work , laissez-passer is a distinguished and distinctive effort by a bona-fide master , a fascinating film replete with rewards to be had by all willing to make the effort to reap them .\\n',\n",
       "  'score': 0.6970540881156921},\n",
       " {'b': \"at about 95 minutes , treasure planet maintains a brisk pace as it races through the familiar story . however , it lacks grandeur and that epic quality often associated with stevenson's tale as well as with earlier disney efforts .\\n\",\n",
       "  'score': 0.6816019415855408},\n",
       " {'b': \"the appearance of treebeard and gollum's expanded role will either have you loving what you're seeing , or rolling your eyes . i loved it ! gollum's 'performance' is incredible !\\n\",\n",
       "  'score': 0.6667085289955139},\n",
       " {'b': 'singer/composer bryan adams contributes a slew of songs \\x97 a few potential hits , a few more simply intrusive to the story \\x97 but the whole package certainly captures the intended , er , spirit of the piece .\\n',\n",
       "  'score': 0.6391994953155518},\n",
       " {'b': \"together , tok and o orchestrate a buoyant , darkly funny dance of death . in the process , they demonstrate that there's still a lot of life in hong kong cinema .\\n\",\n",
       "  'score': 0.6251924633979797},\n",
       " {'b': \"the story loses its bite in a last-minute happy ending that's even less plausible than the rest of the picture . much of the way , though , this is a refreshingly novel ride .\\n\",\n",
       "  'score': 0.6126499772071838},\n",
       " {'b': 'i enjoyed time of favor while i was watching it , but i was surprised at how quickly it faded from my memory .\\n',\n",
       "  'score': 0.61087566614151},\n",
       " {'b': 'director kapur is a filmmaker with a real flair for epic landscapes and adventure , and this is a better film than his earlier english-language movie , the overpraised elizabeth .\\n',\n",
       "  'score': 0.606583297252655},\n",
       " {'b': \"steers turns in a snappy screenplay that curls at the edges ; it's so clever you want to hate it . but he somehow pulls it off .\\n\",\n",
       "  'score': 0.6064082384109497},\n",
       " {'b': \"some actors have so much charisma that you'd be happy to listen to them reading the phone book . hugh grant and sandra bullock are two such likeable actors .\\n\",\n",
       "  'score': 0.5922940373420715},\n",
       " {'b': 'like most bond outings in recent years , some of the stunts are so outlandish that they border on being cartoonlike . a heavy reliance on cgi technology is beginning to creep into the series .\\n',\n",
       "  'score': 0.5764650702476501},\n",
       " {'b': \"it might be tempting to regard mr . andrew and his collaborators as oddballs , but mr . earnhart's quizzical , charming movie allows us to see them , finally , as artists .\\n\",\n",
       "  'score': 0.5718626976013184},\n",
       " {'b': 'edited and shot with a syncopated style mimicking the work of his subjects , pray turns the idea of the documentary on its head , making it rousing , invigorating fun lacking any mtv puffery .\\n',\n",
       "  'score': 0.5694429874420166},\n",
       " {'b': 'katz uses archival footage , horrifying documents of lynchings , still photographs and charming old reel-to-reel recordings of meeropol entertaining his children to create his song history , but most powerful of all is the song itself\\n',\n",
       "  'score': 0.5642381906509399},\n",
       " {'b': 'with a cast that includes some of the top actors working in independent film , lovely & amazing involves us because it is so incisive , so bleakly amusing about how we go about our lives .\\n',\n",
       "  'score': 0.558781623840332},\n",
       " {'b': 'karmen moves like rhythm itself , her lips chanting to the beat , her long , braided hair doing little to wipe away the jeweled beads of sweat .\\n',\n",
       "  'score': 0.5567308664321899},\n",
       " {'b': \"a compelling coming-of-age drama about the arduous journey of a sensitive young girl through a series of foster homes and a fierce struggle to pull free from her dangerous and domineering mother's hold over her .\\n\",\n",
       "  'score': 0.5561511516571045},\n",
       " {'b': \"while it would be easy to give crush the new title of two weddings and a funeral , it's a far more thoughtful film than any slice of hugh grant whimsy .\\n\",\n",
       "  'score': 0.5525131821632385},\n",
       " {'b': 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson\\'s expanded vision of j . r . r . tolkien\\'s middle-earth .\\n',\n",
       "  'score': 0.535361111164093},\n",
       " {'b': \"on the surface , it's a lovers-on-the-run crime flick , but it has a lot in common with piesiewicz's and kieslowski's earlier work , films like the double life of veronique .\\n\",\n",
       "  'score': 0.5041318535804749},\n",
       " {'b': 'the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\\n',\n",
       "  'score': 0.4721093773841858},\n",
       " {'b': 'whether or not you\\'re enlightened by any of derrida\\'s lectures on \" the other \" and \" the self , \" derrida is an undeniably fascinating and playful fellow .\\n',\n",
       "  'score': 0.433889776468277},\n",
       " {'b': \"doug liman , the director of bourne , directs the traffic well , gets a nice wintry look from his locations , absorbs us with the movie's spycraft and uses damon's ability to be focused and sincere .\\n\",\n",
       "  'score': 0.4172232747077942}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# LogisticRegression\n",
    "\n",
    "# # scores_df.sort_values('score', ascending=False)[['b', 'score']].to_dict('records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
