{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae85db3b",
   "metadata": {},
   "source": [
    "# todo\n",
    "- Проверить на задаче классификации отзывов что получаемые эмбединги текстов - норм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfe08866",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d8caced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b030f6",
   "metadata": {},
   "source": [
    "# Masking language modelling MLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85bf82f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting masking_language_modelling/models.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile masking_language_modelling/models.py\n",
    "\n",
    "import torch\n",
    "\n",
    "from multihead_attention import MultiHeadAttention\n",
    "\n",
    "class BaseEncoderModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, n_heads, emb_size, vdim=None, kdim=None, padding_idx=None):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Embedding(vocab_size, emb_size, padding_idx=padding_idx),\n",
    "            MultiHeadAttention(n_heads=n_heads, emb_size=emb_size),\n",
    "            MultiHeadAttention(n_heads=n_heads, emb_size=emb_size),\n",
    "        )\n",
    "    def forward(self, X: torch.Tensor):\n",
    "        return self.layers(X)\n",
    "\n",
    "\n",
    "class MLMHead(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        out_features = self.model.layers[-1].ffn.inplace.out_features\n",
    "        vocab_size = self.model.layers[0].num_embeddings\n",
    "        self.mlm_layer = torch.nn.Linear(out_features, vocab_size, bias=False)\n",
    "        \n",
    "    def forward(self, X: torch.Tensor): #, masked_tokens: torch.Tensor): \n",
    "        state = self.model(X)\n",
    "        logits = self.mlm_layer(state)\n",
    "        # you can't truncate only masked tokens here because the shape of the bantch will be broken.\n",
    "        #         masked_tokens_logits = logits[:, masked_tokens, :]\n",
    "#         result = torch.softmax(logits, axis=-1) # will add this in loss\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3af277f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 7])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, seq_len, emb_size = 2, 3, 7\n",
    "vocab_size = 3\n",
    "torch.nn.Embedding(vocab_size, emb_size)(torch.randint(vocab_size, size=(batch_size, seq_len))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41eb1878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 47, 1000])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from masking_language_modelling.models import BaseEncoderModel, MLMHead\n",
    "from masking_language_modelling.dataproc import MLMDataset\n",
    "# batch_size, seq_len, emb_size = 11, 30, 36\n",
    "# X = torch.Tensor(batch_size, seq_len, emb_size).random_()\n",
    "\n",
    "\n",
    "\n",
    "batch_size, seq_len, emb_size = 11, 47, 36\n",
    "vocab_size = 1000\n",
    "base_model = BaseEncoderModel(vocab_size=vocab_size, n_heads=12, emb_size=emb_size)\n",
    "model = MLMHead(model=base_model)\n",
    "\n",
    "X = torch.randint(vocab_size, size=(batch_size, seq_len))\n",
    "probs = model(X)\n",
    "probs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26462494",
   "metadata": {},
   "source": [
    "# Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b22a1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting masking_language_modelling/dataproc.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile masking_language_modelling/dataproc.py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from typing import List, Optional\n",
    "\n",
    "class MLMDataset(Dataset):\n",
    "    def __init__(self, text_fpath: str, \n",
    "                 max_seq_len: int, \n",
    "                 mask_ratio:float = 0.15\n",
    "                ):\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.mask_ratio = mask_ratio\n",
    "        with open(text_fpath, 'r') as f:\n",
    "            self.lines = f.readlines()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.lines)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.lines[idx]\n",
    "    \n",
    "import string\n",
    "import re\n",
    "\n",
    "class Tokenizer:\n",
    "    punctuation = string.punctuation.replace('-', '')\n",
    "    def __init__(self, \n",
    "                 max_vocab_size: int,\n",
    "                 truncation: bool = True,\n",
    "                 max_seq_len: Optional[int] = None,\n",
    "                 padding: bool = True,\n",
    "                ):\n",
    "        self.pad_token = '<PAD>'\n",
    "        self.mask_token = '<MASK>'\n",
    "        self.cls_token = '<CLS>'\n",
    "        self.sep_token = '<SEP>'\n",
    "        self.special_tokens = [self.cls_token, self.sep_token, self.pad_token, self.mask_token]\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.truncation = truncation\n",
    "        self.padding = padding\n",
    "        \n",
    "        if self.padding or self.truncation:\n",
    "            assert not(max_seq_len is None)\n",
    "            self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def fit(self, dataset):\n",
    "        most_common_words = Counter(chain.from_iterable(map(self._preproc, dataset)))\\\n",
    "                                                     .most_common(self.max_vocab_size-len(self.special_tokens))\n",
    "        most_common_words = list(map(lambda x: x[0], most_common_words))\n",
    "        self.vocab = dict(map(lambda x: (x[1], x[0]), enumerate(self.special_tokens + most_common_words)))\n",
    "        return self\n",
    "    \n",
    "    def apply(self, text: str):\n",
    "        pad_token_idx = self.vocab[self.pad_token]\n",
    "        input_seq = self._preproc(text)[:self.max_seq_len]\n",
    "        payload_tokens = list(map(lambda x: self.vocab.get(x, pad_token_idx), input_seq))\n",
    "        padding_tokens = [pad_token_idx]*(self.max_seq_len-len(payload_tokens))\n",
    "        return [self.vocab[self.cls_token]] + payload_tokens + padding_tokens\n",
    "    \n",
    "    def _preproc(self, text: str) -> List[str]:\n",
    "        sub_pattern = f'[{re.escape(Tokenizer.punctuation)}]'\n",
    "        return re.sub(sub_pattern, ' ', text.lower()).split()\n",
    "    \n",
    "\n",
    "def spawn_collate_fn(tokenizer, mask_ratio=0.15):\n",
    "    cls_token_id = tokenizer.vocab[tokenizer.cls_token]\n",
    "    sep_token_id = tokenizer.vocab[tokenizer.sep_token]\n",
    "    mask_token_id = tokenizer.vocab[tokenizer.mask_token]\n",
    "    \n",
    "    def mask_objective(batch_token_ids, mask_ratio):\n",
    "            masked_tokens = torch.rand(batch_token_ids.shape)<mask_ratio\n",
    "            mask_arr = masked_tokens * (batch_token_ids != cls_token_id) * (batch_token_ids != sep_token_id)\n",
    "            return mask_arr\n",
    "        \n",
    "    def custom_collate_fn(batch):\n",
    "        input_ids = torch.Tensor(batch).long()\n",
    "        mlm_mask = mask_objective(input_ids, mask_ratio)\n",
    "        masked_input_ids = torch.where(mlm_mask, mask_token_id, input_ids).long()\n",
    "\n",
    "        return {\n",
    "            'input_tokens': input_ids,\n",
    "            'masked_input_tokens': masked_input_ids,\n",
    "#             'attention_mask': 1, # is this the same as mlm mask?\n",
    "            'mlm_mask': mlm_mask,\n",
    "        }\n",
    "    return custom_collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e847c06",
   "metadata": {},
   "source": [
    "# Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d54d506f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile test.txt\n",
    "kek\n",
    "mda ok na\n",
    "aagaaa\n",
    "kek\n",
    "mda ok na\n",
    "aagaaa\n",
    "kek\n",
    "mda ok na\n",
    "aagaaa\n",
    "kek\n",
    "mda ok na\n",
    "aagaaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db780afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset imdb (/home/grigory/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38ce906e7945428c869770e6b94b9c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from itertools import chain\n",
    "from datasets import load_dataset\n",
    "# tmp_dataset = load_dataset(\"rotten_tomatoes\")\n",
    "# # with open('rt.txt', 'w') as f:\n",
    "# #     for x in tmp_dataset['train']:\n",
    "# #         if len(x['text'].split())<10:\n",
    "# #             continue\n",
    "# #         f.write(x['text']+'\\n')\n",
    "\n",
    "tmp_dataset = load_dataset(\"imdb\")\n",
    "with open('imdb_100k.txt', 'w') as f:\n",
    "    for x in tmp_dataset['unsupervised']:\n",
    "        if len(x['text'].split())<10:\n",
    "            continue\n",
    "        f.write(x['text']+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b88f9f5",
   "metadata": {},
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a7983054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from masking_language_modelling.dataproc import MLMDataset, spawn_collate_fn, Tokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "seq_len = 30\n",
    "batch_size = 50\n",
    "max_vocab_size = 30000\n",
    "\n",
    "training_data = MLMDataset('imdb_100k.txt', seq_len)\n",
    "tokenizer = Tokenizer(max_vocab_size=max_vocab_size, max_seq_len=seq_len)\\\n",
    "                        .fit(training_data)\n",
    "\n",
    "# print(tokenizer.apply(f'kek mda {tokenizer.mask_token}'))\n",
    "proc_train_dataset = list(map(tokenizer.apply, training_data))\n",
    "\n",
    "collate_fn = spawn_collate_fn(tokenizer)\n",
    "train_dataloader = DataLoader(proc_train_dataset, batch_size=batch_size, shuffle=True, \n",
    "                              collate_fn=collate_fn)\n",
    "# next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f19a0c6",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9378fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 54\n",
    "\n",
    "base_model = BaseEncoderModel(n_heads=3, emb_size=emb_size, \n",
    "                              vocab_size=len(tokenizer.vocab),\n",
    "                              padding_idx=tokenizer.vocab[tokenizer.pad_token])\n",
    "model = MLMHead(model=base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e97fa820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5494b8e0ec49cb9756e3c52c82777d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c039db9785254f2382f7606daf058946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd92c7e44594d8c81a2d9f887c4d64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4d479d2c474bd8b0c6318ff6064586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e66e5f81a7c4450a19264b689dd4b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0db5cae210040c7a54940ad3d6cb2d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b60aed8ade6417681f07d4b83bdc65b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b120688b1ea452599ce16a9126e5859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e281bf58422747dcaba586bb04b1cb22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee78f09d4ab4e079fdce42f4d8bbaa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a151eda59296462a94093f69909311a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46601c302bfb44f88440e5bf0000eac7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f15006de4a4f99acb073a53769b6c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01ac8ac5bd324dc19eeadfee656d0b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55f1291b0c20473ba9e2ec4e689ee8ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae0b41ac71804a0099d5ef08c73ca112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aca1b0348e62476f8932b7867f80dec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b5d4d772a04e6988cfd40bec80b47e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf8f71b20aa449fd99e114aebb380bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe5464d80b5444b933b852ee48048fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m             losses\u001b[38;5;241m.\u001b[39mappend(avg_masked_loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     29\u001b[0m             pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCEL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(losses)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, num_epochs, lr)\u001b[0m\n\u001b[1;32m     24\u001b[0m masked_loss \u001b[38;5;241m=\u001b[39m loss_res\u001b[38;5;241m*\u001b[39mmasked_tokens\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     25\u001b[0m avg_masked_loss \u001b[38;5;241m=\u001b[39m masked_loss\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m---> 26\u001b[0m \u001b[43mavg_masked_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     28\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(avg_masked_loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/miniconda3/envs/research38/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/research38/lib/python3.8/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def train(model, train_dataloader, num_epochs=1, lr=1e-3):    \n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "    for epoch in range(num_epochs):\n",
    "        losses = []\n",
    "        print(f'Epoch {epoch}')\n",
    "        pbar=tqdm(train_dataloader)\n",
    "        for batch in pbar:\n",
    "            # x is tensor with masked tokens [batch_size, seq_len]\n",
    "            x = batch['masked_input_tokens']\n",
    "\n",
    "            # y is tensor without masked_tokens [batch_size, seq_len]\n",
    "            y = batch['input_tokens']\n",
    "\n",
    "            # masked_tokens is boolean tensor with masked tokens [batch_size, seq_len]\n",
    "            masked_tokens = batch['mlm_mask']\n",
    "            model.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss_res = loss(logits.view(-1,logits.shape[2]), y.view(-1))\n",
    "            masked_loss = loss_res*masked_tokens.view(-1)\n",
    "            avg_masked_loss = masked_loss.mean()\n",
    "            avg_masked_loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(avg_masked_loss.detach().numpy())\n",
    "            pbar.set_description(f'CEL: {np.mean(losses):.3f}')\n",
    "        \n",
    "train(model, train_dataloader, num_epochs=100, lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db7b5f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "def save_state(model, tokenizer, save_path):\n",
    "    # will use joblib for simplicity\n",
    "    state = {\n",
    "        'tokenizer': tokenizer,\n",
    "        'model': model\n",
    "    }\n",
    "    joblib.dump(state, save_path)\n",
    "    \n",
    "save_state(model, tokenizer, 'long_run_model.j')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49133f3",
   "metadata": {},
   "source": [
    "sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "56f2d798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>fairytale for yound ladies. miracle fantasy</td>\n",
       "      <td>La Moustache opens with a man thinking about p...</td>\n",
       "      <td>0.882530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>fairytale for yound ladies. miracle fantasy</td>\n",
       "      <td>Great movie, great cast. Why an American and a...</td>\n",
       "      <td>0.878254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>fairytale for yound ladies. miracle fantasy</td>\n",
       "      <td>Old Bobby De Niro strolls through this film wi...</td>\n",
       "      <td>0.875376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>fairytale for yound ladies. miracle fantasy</td>\n",
       "      <td>The Blues Brothers franchise self destructs. W...</td>\n",
       "      <td>0.857263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>fairytale for yound ladies. miracle fantasy</td>\n",
       "      <td>This sequel picks up shortly after the first f...</td>\n",
       "      <td>0.853891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>fairytale for yound ladies. miracle fantasy</td>\n",
       "      <td>I gave this a ten in a futile attempt to up th...</td>\n",
       "      <td>0.167051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>786</th>\n",
       "      <td>fairytale for yound ladies. miracle fantasy</td>\n",
       "      <td>i wanted to like this movie,and i really tied....</td>\n",
       "      <td>0.166706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>672</th>\n",
       "      <td>fairytale for yound ladies. miracle fantasy</td>\n",
       "      <td>I could tear this piece of crap apart, frame b...</td>\n",
       "      <td>0.145186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>fairytale for yound ladies. miracle fantasy</td>\n",
       "      <td>Sorry folks, but I had some real problems with...</td>\n",
       "      <td>0.141119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>682</th>\n",
       "      <td>fairytale for yound ladies. miracle fantasy</td>\n",
       "      <td>I enjoyed this much more than I expected to. D...</td>\n",
       "      <td>0.105942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               a  \\\n",
       "841  fairytale for yound ladies. miracle fantasy   \n",
       "966  fairytale for yound ladies. miracle fantasy   \n",
       "688  fairytale for yound ladies. miracle fantasy   \n",
       "410  fairytale for yound ladies. miracle fantasy   \n",
       "262  fairytale for yound ladies. miracle fantasy   \n",
       "..                                           ...   \n",
       "380  fairytale for yound ladies. miracle fantasy   \n",
       "786  fairytale for yound ladies. miracle fantasy   \n",
       "672  fairytale for yound ladies. miracle fantasy   \n",
       "808  fairytale for yound ladies. miracle fantasy   \n",
       "682  fairytale for yound ladies. miracle fantasy   \n",
       "\n",
       "                                                     b     score  \n",
       "841  La Moustache opens with a man thinking about p...  0.882530  \n",
       "966  Great movie, great cast. Why an American and a...  0.878254  \n",
       "688  Old Bobby De Niro strolls through this film wi...  0.875376  \n",
       "410  The Blues Brothers franchise self destructs. W...  0.857263  \n",
       "262  This sequel picks up shortly after the first f...  0.853891  \n",
       "..                                                 ...       ...  \n",
       "380  I gave this a ten in a futile attempt to up th...  0.167051  \n",
       "786  i wanted to like this movie,and i really tied....  0.166706  \n",
       "672  I could tear this piece of crap apart, frame b...  0.145186  \n",
       "808  Sorry folks, but I had some real problems with...  0.141119  \n",
       "682  I enjoyed this much more than I expected to. D...  0.105942  \n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy import spatial\n",
    "\n",
    "\n",
    "def get_embedding(query, tokenizer, base_model):\n",
    "    tokens = tokenizer.apply(query)\n",
    "    return base_model(torch.tensor(tokens).view(1, -1))[0, 0].detach().numpy()\n",
    "\n",
    "queries = list(training_data)[:1000]\n",
    "embs = []\n",
    "base_model.eval()\n",
    "\n",
    "query = 'fairytale for yound ladies. miracle fantasy'\n",
    "query_emb = get_embedding(query, tokenizer, base_model)\n",
    "\n",
    "res_lst = []\n",
    "for entry in queries:\n",
    "    entry_emb = get_embedding(entry, tokenizer, base_model)\n",
    "    score = 1 - spatial.distance.cosine(query_emb, entry_emb)\n",
    "    res_lst.append((query, entry, score))\n",
    "\n",
    "scores_df = pd.DataFrame(res_lst, columns=['a', 'b', 'score'])\n",
    "scores_df.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "17dd50d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.1464)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# baseline loss\n",
    "kek_loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "n, C = 5, len(tokenizer.vocab)\n",
    "\n",
    "a1 = torch.Tensor(np.random.uniform(size=(n, C)))\n",
    "b1 = torch.Tensor(np.random.randint(C, size=n)).long()\n",
    "kek_loss(a1, b1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc12bea2",
   "metadata": {},
   "source": [
    "## Estimate quliaty with review classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bb1ee80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "loaded_state = joblib.load('long_run_model.j')\n",
    "l_model, l_tokenizer = loaded_state['model'], loaded_state['tokenizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf618e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset rotten_tomatoes (/home/grigory/.cache/huggingface/datasets/rotten_tomatoes/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e776a78e170a4d8c9a908bc899a892da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "tmp_dataset = load_dataset(\"rotten_tomatoes\")\n",
    "train_df, val_df, test_df = pd.DataFrame(tmp_dataset['train']), \\\n",
    "                            pd.DataFrame(tmp_dataset['validation']), \\\n",
    "                            pd.DataFrame(tmp_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fff482bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def batch_emb(inputs, bs = 500):\n",
    "    train_embs = []\n",
    "    inputs_p = inputs.text.apply(l_tokenizer.apply).tolist()\n",
    "    n = len(inputs_p)\n",
    "    for i in range(0, n, bs):\n",
    "        yield l_model.model(torch.Tensor(inputs_p[i:i+bs]).long())[:, 0, :].detach().numpy()\n",
    "    \n",
    "train_emb = np.concatenate(list(batch_emb(train_df)), axis=0)\n",
    "val_emb = np.concatenate(list(batch_emb(val_df)), axis=0)\n",
    "test_emb = np.concatenate(list(batch_emb(test_df)), axis=0)\n",
    "\n",
    "y_tr, y_val, y_te = train_df.label.values, \\\n",
    "                    val_df.label.values, \\\n",
    "                    test_df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69994d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6495478478138671, 0.6421825554667727, 0.6459137805406052)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "lr = LogisticRegression(max_iter=2000).fit(train_emb, y_tr)\n",
    "# lr = KNeighborsClassifier(n_neighbors=3).fit(train_emb, y_tr)\n",
    "# lr = LGBMClassifier(n_estimators=500, max_depth=5, learning_rate=1e-3).fit(train_emb, y_tr)\n",
    "roc_auc_score(y_true=y_tr, y_score=lr.predict_proba(train_emb)[:, 1]),\\\n",
    "roc_auc_score(y_true=y_te, y_score=lr.predict_proba(test_emb)[:, 1]),\\\n",
    "roc_auc_score(y_true=y_val, y_score=lr.predict_proba(val_emb)[:, 1]),\\\n",
    "\n",
    "\n",
    "# scores_df.sort_values('score', ascending=False)[['b', 'score']].to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a4441ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9601955445850725, 0.859547536159443, 0.8289444505067074)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "tfidf = TfidfVectorizer().fit(train_df.text)\n",
    "tfidf_tr_emb = tfidf.transform(train_df.text)\n",
    "tfidf_val_emb = tfidf.transform(val_df.text)\n",
    "tfidf_te_emb = tfidf.transform(test_df.text)\n",
    "\n",
    "\n",
    "lr = LogisticRegression(max_iter=2000).fit(tfidf_tr_emb, y_tr)\n",
    "roc_auc_score(y_true=y_tr, y_score=lr.predict_proba(tfidf_tr_emb)[:, 1]),\\\n",
    "roc_auc_score(y_true=y_te, y_score=lr.predict_proba(tfidf_te_emb)[:, 1]),\\\n",
    "roc_auc_score(y_true=y_val, y_score=lr.predict_proba(tfidf_val_emb)[:, 1]),\\\n",
    "\n",
    "\n",
    "# scores_df.sort_values('score', ascending=False)[['b', 'score']].to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592b5a22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
